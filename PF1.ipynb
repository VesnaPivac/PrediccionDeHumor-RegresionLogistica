{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNwrP0PdnL7G"
      },
      "source": [
        "# Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wR7VaFQV6o5m"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerias necesarias para el correcto funcionamiento del codigo\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LostjXxMeBS"
      },
      "source": [
        "# Pre-procesamiento de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pkWdHtze6l9s"
      },
      "outputs": [],
      "source": [
        "#Creamos una variable df con el dataset\n",
        "df = pd.read_csv(\"haha_2021_train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8q1WuYVGGe1c"
      },
      "outputs": [],
      "source": [
        "df.drop([\"votes_no\",\"votes_1\", \"votes_2\", \"votes_3\", \"votes_4\", \"votes_5\",\"humor_mechanism\",\"humor_target\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NT4qfcb6yOZ",
        "outputId": "ae3f4484-9dec-4fa0-d873-4fc66910a444"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\te512362\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6otAfqYl62px"
      },
      "outputs": [],
      "source": [
        "stop_words_sp = set(stopwords.words('spanish'))\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "#Concatenar las stopwords aplicándose a una cuenta que genera contenido en inglés y español\n",
        "stop_words = stop_words_sp | stop_words_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rGsKMFjsUz6X"
      },
      "outputs": [],
      "source": [
        "def LimpiarTexto(tweet):\n",
        "    stopwords_english = stopwords.words('spanish')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # quita el \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet) \n",
        "    # quita hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet) \n",
        "    # quita hashtags dejando la palabra sin el signo # \n",
        "    tweet = re.sub(r'#', '', tweet) \n",
        "    tweet = re.sub(r'[^\\w\\s]','',tweet) #Signos de puntuacion \n",
        "    tweet = tweet.lower() #Minusculas\n",
        "    tweet = tweet.replace(\"ñ\", \"#\") #Reemplaza 'ñ'\n",
        "    tweet = unicodedata.normalize(\"NFKD\", tweet)\\\n",
        "    .encode(\"ascii\",\"ignore\").decode(\"ascii\")\\\n",
        "    .replace(\"#\", \"ñ\") #Quita acentos\n",
        "   \n",
        "    eliminar_stopwords(tweet, stopwords_english)\n",
        " \n",
        "    return tweet\n",
        "\n",
        "def eliminar_stopwords(texto, stopwords):\n",
        "    return ' '.join([word for word in texto.split(' ') if word not in stopwords])\n",
        "\n",
        "def Tokenizacion(tweet):\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "  return tweet_tokens\n",
        "\n",
        "def Stemming(tweet):\n",
        "  stemmer = SnowballStemmer(\"spanish\")\n",
        "  tweets_clean = []\n",
        "  for word in tweet:\n",
        "      stem_word = stemmer.stem(word)  # stemming word\n",
        "      tweets_clean.append(stem_word)\n",
        "\n",
        "  return tweets_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8Ksc8BBDU3ED"
      },
      "outputs": [],
      "source": [
        "df = df.assign(CleanText=\"\",TokenizeText=\"\", Text=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd3Vg1bAVi8O",
        "outputId": "998310db-7a29-403d-94cc-69fdcade8363"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\te512362\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "c:\\Users\\te512362\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "c:\\Users\\te512362\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(df)):\n",
        "  df[\"CleanText\"][i]  = LimpiarTexto(df['text'][i])\n",
        "  df[\"TokenizeText\"][i] = Tokenizacion(df['CleanText'][i])\n",
        "  df[\"Text\"][i] = Stemming(df['TokenizeText'][i])\n",
        "\n",
        "df['TokenizeText'] = df.TokenizeText.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CWVu5iXdhDFq"
      },
      "outputs": [],
      "source": [
        "df = df.assign(Corpus=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUpkD3NHg1o8",
        "outputId": "ed7c782c-5ac7-4dd9-a7c2-59d7915a0462"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\te512362\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "x = []\n",
        "for i in range(len(df)):\n",
        "  x =\"\".join(df['TokenizeText'][i])\n",
        "  df[\"Corpus\"][i] = x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoAqdzi1r5tA"
      },
      "source": [
        "# Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kySsHGmnr9rI"
      },
      "outputs": [],
      "source": [
        "#Divide los datos en train y test\n",
        "from sklearn.model_selection import train_test_split\n",
        "y = df['is_humor']\n",
        "x = df['Corpus']\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-L3JmrknsSkm"
      },
      "outputs": [],
      "source": [
        "Encoder = LabelEncoder()\n",
        "Train_Y = Encoder.fit_transform(train_y)\n",
        "Test_Y = Encoder.fit_transform(test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "track_uri = \"http://localhost:5000/\" # Esto puede ser que cambie por http://0.0.0.0:1234\n",
        "mlflow.set_tracking_uri(track_uri)\n",
        "mlflow.set_registry_uri(\"sqlite:////tmp/registry.db\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLflow Version: 1.23.1\n",
            "Tracking URI: http://localhost:5000/\n",
            "Nombre del experimento: Topicos\n",
            "ID del experimento: 2\n"
          ]
        }
      ],
      "source": [
        "# Generando el experimento o cargandolo si existe\n",
        "experiment_name = \"Topicos\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "# Cargando la información\n",
        "client = mlflow.tracking.MlflowClient()\n",
        "experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
        "\n",
        "\n",
        "# Vamos a ver si es cierto\n",
        "print(f\"MLflow Version: {mlflow.__version__}\")\n",
        "print(f\"Tracking URI: {mlflow.tracking.get_tracking_uri()}\")\n",
        "print(f\"Nombre del experimento: {experiment_name}\")\n",
        "print(f\"ID del experimento: {experiment_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrgJJ-CL8g-j",
        "outputId": "7457f61d-38ab-4830-9127-7f57c974b658"
      },
      "outputs": [],
      "source": [
        "mlflow.sklearn.autolog()\n",
        "ngram_r = (1,2)\n",
        "max_f = 100\n",
        "Tfidf_vect = TfidfVectorizer(ngram_range=ngram_r, max_features=max_f)\n",
        "Tfidf_vect.fit(df['Corpus'])\n",
        "Train_X_Tfidf = Tfidf_vect.transform(train_x)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(test_x)\n",
        "len(Tfidf_vect.vocabulary_)\n",
        "\n",
        "\n",
        "with open('tfidf_vect.pkl', 'wb') as file:  \n",
        "    pickle.dump(Tfidf_vect, file)\n",
        "\n",
        "params = {'solver':'lbfgs'}\n",
        "mlflow.log_params(params)\n",
        "mlflow.log_artifact(\"tfidf_vect.pkl\", \"Vectores\")\n",
        "mlflow.log_metrics(\n",
        "    {'ngram_param_1': ngram_r[0],'ngram_param_2':ngram_r[1] , 'max_features':max_f}\n",
        ")\n",
        "\n",
        "modelLogisticRegression = LogisticRegression(**params)\n",
        "\n",
        "modelLogisticRegression.fit(Train_X_Tfidf,train_y)\n",
        "\n",
        "# mlflow.sklearn.log_model(modelLogisticRegression, artifact_path=\"sklearn-model\")\n",
        "\n",
        "metrics = mlflow.sklearn.eval_and_log_metrics(modelLogisticRegression, Test_X_Tfidf, test_y, prefix=\"val_\")\n",
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp_YeWeB-dwx",
        "outputId": "32880d9f-5589-45da-8f2e-b0fa98e5e353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1], dtype=int64)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = 'Van dos ciegos y le dice uno al otro: -Ojala lloviera... -Ojala yo tambien...'\n",
        "prediccion_X = Tfidf_vect.transform([X])\n",
        "modelLogisticRegression.predict(prediccion_X)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "91LlpJ5Wm14O",
        "f4gJNI43m9pJ"
      ],
      "name": "PF1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
